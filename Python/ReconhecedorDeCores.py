import numpy as np
import os
from PIL import Image
import matplotlib.pyplot as plt
import random

# Esta classe implementa todas as operaÃ§Ãµes matemÃ¡ticas necessÃ¡rias
class RedeNeuralCores:

        # CONSTRUTOR: Inicializa a rede neural
        # PARÃ‚METROS:
        # tamanho_entrada: 784 (28x28 pixels da imagem)
        # camada1: 64 neurÃ´nios na primeira camada oculta
        # camada2: 32 neurÃ´nios na segunda camada oculta
        # num_classes: 4 classes de cores
        # taxa_aprendizado: velocidade de aprendizado

    def __init__(self, tamanho_entrada=784, camada1=64, camada2=32, num_classes=4, taxa_aprendizado=0.01):
        self.taxa_aprendizado = taxa_aprendizado
        
        # INICIALIZAÃ‡ÃƒO DOS PESOS (WEIGHTS)
        # Pesos da camada de entrada para primeira camada oculta
        self.W1 = np.random.randn(tamanho_entrada, camada1) * np.sqrt(2.0 / tamanho_entrada)
        self.b1 = np.zeros((1, camada1))  # Bias da primeira camada
        
        # Pesos da primeira para segunda camada oculta
        self.W2 = np.random.randn(camada1, camada2) * np.sqrt(2.0 / camada1)
        self.b2 = np.zeros((1, camada2))  # Bias da segunda camada
        
        # Pesos da segunda camada oculta para saÃ­da
        self.W3 = np.random.randn(camada2, num_classes) * np.sqrt(2.0 / camada2)
        self.b3 = np.zeros((1, num_classes))  # Bias da camada de saÃ­da

        print(f"Rede inicializada: {tamanho_entrada} -> {camada1} -> {camada2} -> {num_classes}")

# FUNÃ‡ÃƒO DE ATIVAÃ‡ÃƒO ReLU (Rectified Linear Unit)
# EXPLICAÃ‡ÃƒO MATEMÃTICA:
# - ReLU(x) = max(0, x)
# - Se x > 0, retorna x
# - Se x â‰¤ 0, retorna 0
    def relu(self, x):
        return np.maximum(0, x)
    
# DERIVADA DA ReLU para backpropagation 
# EXPLICAÃ‡ÃƒO MATEMÃTICA:
# - Se x > 0, derivada = 1
# - Se x â‰¤ 0, derivada = 0
# - NecessÃ¡ria para calcular gradientes no treinamento
    
    def relu_derivada(self, x):
        return (x > 0).astype(float)

# FUNÃ‡ÃƒO DE ATIVAÃ‡ÃƒO SOFTMAX para camada de saÃ­da
        
# EXPLICAÃ‡ÃƒO MATEMÃTICA:
# - Converte nÃºmeros em probabilidades que somam 1
# - softmax(x_i) = exp(x_i) / sum(exp(x_j) for all j)
# - Ideal para classificaÃ§Ã£o multiclasse
        
# EXEMPLO:
# Entrada: [2.0, 1.0, 0.1, 3.0]
# - SaÃ­da: [0.24, 0.09, 0.03, 0.64] (soma = 1.0)
    def softmax(self, x):
        # SubtraÃ­mos o mÃ¡ximo para estabilidade numÃ©rica (evita overflow)
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def forward_pass(self, X):
        """
        FORWARD PASS: Propaga dados da entrada atÃ© a saÃ­da
        
        EXPLICAÃ‡ÃƒO PASSO A PASSO:
        1. Entrada X (imagens normalizadas) entra na rede
        2. Primeira camada: X * W1 + b1, depois ReLU
        3. Segunda camada: resultado * W2 + b2, depois ReLU  
        4. SaÃ­da: resultado * W3 + b3, depois Softmax
        
        MATEMÃTICA:
        - z = X @ W + b (multiplicaÃ§Ã£o matricial + bias)
        - a = activation_function(z)
        """
        # Primeira camada oculta
        self.z1 = X @ self.W1 + self.b1  # CombinaÃ§Ã£o linear
        self.a1 = self.relu(self.z1)     # AtivaÃ§Ã£o ReLU
        
        # Segunda camada oculta
        self.z2 = self.a1 @ self.W2 + self.b2  # CombinaÃ§Ã£o linear
        self.a2 = self.relu(self.z2)           # AtivaÃ§Ã£o ReLU
        
        # Camada de saÃ­da
        self.z3 = self.a2 @ self.W3 + self.b3  # CombinaÃ§Ã£o linear
        self.a3 = self.softmax(self.z3)        # AtivaÃ§Ã£o Softmax
        
        return self.a3
    
    def calcular_perda(self, y_pred, y_true):
        """
        FUNÃ‡ÃƒO DE PERDA: Cross-Entropy Loss
        
        EXPLICAÃ‡ÃƒO MATEMÃTICA:
        - Loss = -sum(y_true * log(y_pred))
        - Penaliza previsÃµes incorretas exponencialmente
        - Ideal para classificaÃ§Ã£o multiclasse
        
        EXEMPLO:
        - Se classe correta tem probabilidade 0.9 â†’ perda baixa
        - Se classe correta tem probabilidade 0.1 â†’ perda alta
        """
        # Evitar log(0) adicionando pequeno epsilon
        epsilon = 1e-15
        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)
        
        # Cross-entropy loss
        return -np.mean(np.sum(y_true * np.log(y_pred_clipped), axis=1))
    
    def backward_pass(self, X, y_true):
        """
        BACKWARD PASS: Backpropagation para calcular gradientes
        
        EXPLICAÃ‡ÃƒO DETALHADA:
        - Calcula como cada peso contribui para o erro
        - Usa regra da cadeia para propagar gradientes
        - Atualiza pesos na direÃ§Ã£o que reduz o erro
        
        MATEMÃTICA:
        - dW = dL/dW (derivada da perda em relaÃ§Ã£o aos pesos)
        - W_novo = W_antigo - taxa_aprendizado * dW
        """
        m = X.shape[0]  # nÃºmero de amostras
        
        # Gradiente da camada de saÃ­da
        dz3 = self.a3 - y_true  # Derivada da softmax com cross-entropy
        dW3 = (self.a2.T @ dz3) / m
        db3 = np.sum(dz3, axis=0, keepdims=True) / m
        
        # Gradiente da segunda camada oculta
        da2 = dz3 @ self.W3.T
        dz2 = da2 * self.relu_derivada(self.z2)
        dW2 = (self.a1.T @ dz2) / m
        db2 = np.sum(dz2, axis=0, keepdims=True) / m
        
        # Gradiente da primeira camada oculta
        da1 = dz2 @ self.W2.T
        dz1 = da1 * self.relu_derivada(self.z1)
        dW1 = (X.T @ dz1) / m
        db1 = np.sum(dz1, axis=0, keepdims=True) / m
        
        # Atualizar pesos e bias (Gradient Descent)
        self.W3 -= self.taxa_aprendizado * dW3
        self.b3 -= self.taxa_aprendizado * db3
        self.W2 -= self.taxa_aprendizado * dW2
        self.b2 -= self.taxa_aprendizado * db2
        self.W1 -= self.taxa_aprendizado * dW1
        self.b1 -= self.taxa_aprendizado * db1
    
    def prever(self, X):
        """
        FUNÃ‡ÃƒO DE PREDIÃ‡ÃƒO: Classifica novas imagens
        
        EXPLICAÃ‡ÃƒO:
        - Executa forward pass
        - Retorna classe com maior probabilidade
        - Usado apÃ³s o treinamento para testar a rede
        """
        probabilidades = self.forward_pass(X)
        return np.argmax(probabilidades, axis=1)
    
    def acuracia(self, X, y_true):
        """
        FUNÃ‡ÃƒO DE AVALIAÃ‡ÃƒO: Calcula precisÃ£o da rede
        
        EXPLICAÃ‡ÃƒO:
        - Compara previsÃµes com rÃ³tulos verdadeiros
        - Retorna percentual de acertos
        - MÃ©trica principal para avaliar performance
        """
        predicoes = self.prever(X)
        y_true_classes = np.argmax(y_true, axis=1)
        return np.mean(predicoes == y_true_classes)

class CarregadorDados:
    """
    CLASSE AUXILIAR: Carrega e processa dataset de imagens
    
    EXPLICAÃ‡ÃƒO:
    - LÃª imagens do diretÃ³rio dataset
    - Converte para arrays numpy
    - Normaliza pixels para [0,1]
    - Cria rÃ³tulos one-hot encoding
    """
    
    def __init__(self, caminho_dataset="../Image/dataset"):
        self.caminho_dataset = caminho_dataset
        self.classes = ["red", "green", "blue", "yellow"]
        self.mapeamento_classes = {classe: i for i, classe in enumerate(self.classes)}
        
    def carregar_imagem(self, caminho_imagem):
        """
        FUNÃ‡ÃƒO: Carrega e processa uma imagem
        
        EXPLICAÃ‡ÃƒO DETALHADA:
        1. Abre imagem com PIL
        2. Converte para RGB (caso seja diferente)
        3. Transforma em array numpy
        4. Normaliza pixels de [0,255] para [0,1]
        5. Achata de (28,28,3) para (2352,) - vetor 1D
        
        POR QUE NORMALIZAR?
        - Pixels vÃ£o de 0 a 255 (inteiros)
        - Redes neurais funcionam melhor com valores [0,1]
        - Evita problemas de gradientes muito grandes
        """
        try:
            # Carregar e converter imagem
            img = Image.open(caminho_imagem).convert('RGB')
            img_array = np.array(img)
            
            # Normalizar pixels para [0,1]
            img_normalizada = img_array.astype(np.float32) / 255.0
            
            # Achatar para vetor 1D
            img_vetor = img_normalizada.flatten()
            
            return img_vetor
        except Exception as e:
            print(f"Erro ao carregar {caminho_imagem}: {e}")
            return None
    
    def carregar_dataset_completo(self):
        """
        FUNÃ‡ÃƒO PRINCIPAL: Carrega todo o dataset
        
        EXPLICAÃ‡ÃƒO DO PROCESSO:
        1. Varre todas as pastas de cores
        2. Carrega cada imagem
        3. Cria arrays X (dados) e y (rÃ³tulos)
        4. Embaralha dados para melhor treinamento
        
        RETORNA:
        - X: array (n_amostras, 2352) com pixels normalizados
        - y: array (n_amostras, 4) com rÃ³tulos one-hot
        """
        print("ğŸ“ Carregando dataset de imagens...")
        
        X = []  # Lista para armazenar imagens
        y = []  # Lista para armazenar rÃ³tulos
        
        for classe in self.classes:
            print(f"   ğŸ“¸ Carregando imagens de {classe}...")
            caminho_classe = os.path.join(self.caminho_dataset, classe)
            
            if not os.path.exists(caminho_classe):
                print(f"   âš ï¸ DiretÃ³rio nÃ£o encontrado: {caminho_classe}")
                continue
            
            # Listar todas as imagens da classe
            arquivos_imagem = [f for f in os.listdir(caminho_classe) if f.endswith('.png')]
            
            for arquivo in arquivos_imagem:
                caminho_completo = os.path.join(caminho_classe, arquivo)
                img_vetor = self.carregar_imagem(caminho_completo)
                
                if img_vetor is not None:
                    X.append(img_vetor)
                    # Criar rÃ³tulo one-hot
                    rotulo = [0] * len(self.classes)
                    rotulo[self.mapeamento_classes[classe]] = 1
                    y.append(rotulo)
            
            print(f"   âœ“ {len(arquivos_imagem)} imagens de {classe} carregadas")
        
        # Converter listas para arrays numpy
        X = np.array(X)
        y = np.array(y)
        
        # Embaralhar dados para melhor treinamento
        indices = np.random.permutation(len(X))
        X = X[indices]
        y = y[indices]
        
        print(f"\nâœ… Dataset carregado com sucesso!")
        print(f"   ğŸ“Š Formato dos dados: {X.shape}")
        print(f"   ğŸ·ï¸  Formato dos rÃ³tulos: {y.shape}")
        print(f"   ğŸ¯ Classes: {self.classes}")
        
        return X, y
    
    def dividir_treino_teste(self, X, y, proporcao_teste=0.2):
        """
        FUNÃ‡ÃƒO: Divide dataset em treino e teste
        
        EXPLICAÃ‡ÃƒO:
        - 80% dos dados para treinamento
        - 20% dos dados para teste
        - Importante para avaliar generalizaÃ§Ã£o da rede
        
        POR QUE DIVIDIR?
        - Treino: rede aprende com estes dados
        - Teste: avalia se rede funciona em dados nunca vistos
        - Evita overfitting (decorar ao invÃ©s de aprender)
        """
        n_teste = int(len(X) * proporcao_teste)
        
        X_teste = X[:n_teste]
        y_teste = y[:n_teste]
        X_treino = X[n_teste:]
        y_treino = y[n_teste:]
        
        print(f"ğŸ“Š DivisÃ£o dos dados:")
        print(f"   ğŸ¯ Treino: {len(X_treino)} amostras")
        print(f"   ğŸ§ª Teste: {len(X_teste)} amostras")
        
        return X_treino, y_treino, X_teste, y_teste

def treinar_rede_neural():
    """
    FUNÃ‡ÃƒO PRINCIPAL DE TREINAMENTO
    
    EXPLICAÃ‡ÃƒO DO PROCESSO COMPLETO:
    1. Carrega dataset de imagens
    2. Divide em treino/teste
    3. Cria e inicializa rede neural
    4. Executa loop de treinamento (Ã©pocas)
    5. Avalia performance final
    """
    print("ğŸš€ INICIANDO TREINAMENTO DA REDE NEURAL")
    print("=" * 60)
    
    # PASSO 1: Carregar dados
    carregador = CarregadorDados()
    X, y = carregador.carregar_dataset_completo()
    
    # PASSO 2: Dividir treino/teste
    X_treino, y_treino, X_teste, y_teste = carregador.dividir_treino_teste(X, y)
    
    # PASSO 3: Criar rede neural
    print("\nğŸ§  Criando rede neural...")
    rede = RedeNeuralCores(
        tamanho_entrada=X.shape[1],  # Tamanho do vetor de pixels
        camada1=64,                  # Primeira camada oculta
        camada2=32,                  # Segunda camada oculta
        num_classes=4,               # 4 cores
        taxa_aprendizado=0.01        # Learning rate
    )
    
    # PASSO 4: Loop de treinamento
    print(f"\nğŸ”„ Iniciando treinamento por {EPOCHS} Ã©pocas...")
    print("-" * 60)
    
    historico_perda = []
    historico_acuracia = []
    
    for epoca in range(EPOCHS):
        # Forward pass
        y_pred = rede.forward_pass(X_treino)
        
        # Calcular perda
        perda = rede.calcular_perda(y_pred, y_treino)
        
        # Backward pass (atualizar pesos)
        rede.backward_pass(X_treino, y_treino)
        
        # Calcular acurÃ¡cia
        acuracia_treino = rede.acuracia(X_treino, y_treino)
        
        # Armazenar mÃ©tricas
        historico_perda.append(perda)
        historico_acuracia.append(acuracia_treino)
        
        # Mostrar progresso a cada 10 Ã©pocas
        if (epoca + 1) % 10 == 0:
            print(f"Ã‰poca {epoca+1:3d}/{EPOCHS} | "
                  f"Perda: {perda:.4f} | "
                  f"AcurÃ¡cia Treino: {acuracia_treino:.3f}")
    
    # PASSO 5: AvaliaÃ§Ã£o final
    print("\n" + "=" * 60)
    print("ğŸ“Š AVALIAÃ‡ÃƒO FINAL DA REDE")
    print("=" * 60)
    
    acuracia_treino_final = rede.acuracia(X_treino, y_treino)
    acuracia_teste_final = rede.acuracia(X_teste, y_teste)
    
    print(f"ğŸ¯ AcurÃ¡cia no Treino: {acuracia_treino_final:.3f} ({acuracia_treino_final*100:.1f}%)")
    print(f"ğŸ§ª AcurÃ¡cia no Teste:  {acuracia_teste_final:.3f} ({acuracia_teste_final*100:.1f}%)")
    
    # Analisar resultado
    if acuracia_teste_final > 0.9:
        print("ğŸ† EXCELENTE! Rede aprendeu muito bem!")
    elif acuracia_teste_final > 0.7:
        print("âœ… BOM! Rede tem performance satisfatÃ³ria!")
    else:
        print("âš ï¸ Rede precisa de mais treinamento ou ajustes...")
    
    return rede, carregador, historico_perda, historico_acuracia

def testar_com_imagem_nova(rede, carregador):
    """
    FUNÃ‡ÃƒO: Testa rede com uma nova imagem
    
    EXPLICAÃ‡ÃƒO:
    - Permite testar a rede com imagens que ela nunca viu
    - Ãštil para validar se realmente aprendeu
    - Mostra as probabilidades de cada classe
    """
    print("\n" + "=" * 60)
    print("ğŸ” TESTE COM NOVA IMAGEM")
    print("=" * 60)
    
    # Criar uma imagem de teste simples
    print("Criando imagem de teste (azul)...")
    
    # Imagem azul 28x28
    img_teste = np.zeros((28, 28, 3))
    img_teste[:, :, 2] = 1.0  # Canal azul = 1.0
    
    # Processar imagem como a rede espera
    img_vetor = img_teste.flatten()
    img_batch = img_vetor.reshape(1, -1)  # Batch de 1 imagem
    
    # Fazer previsÃ£o
    probabilidades = rede.forward_pass(img_batch)[0]
    classe_prevista = np.argmax(probabilidades)
    
    print(f"\nğŸ“Š RESULTADO DA PREVISÃƒO:")
    print(f"   ğŸ¯ Classe prevista: {carregador.classes[classe_prevista]}")
    print(f"   ğŸ“ˆ ConfianÃ§a: {probabilidades[classe_prevista]:.3f}")
    
    print(f"\nğŸ“‹ Probabilidades por classe:")
    for i, classe in enumerate(carregador.classes):
        print(f"   {classe:8s}: {probabilidades[i]:.3f}")

# PARÃ‚METROS DE TREINAMENTO
EPOCHS = 100  # NÃºmero de Ã©pocas de treinamento

if __name__ == "__main__":
    """
    EXECUÃ‡ÃƒO PRINCIPAL DO PROGRAMA
    
    FLUXO COMPLETO:
    1. Treina a rede neural
    2. Testa com nova imagem
    3. Mostra resultados
    """
    # Treinar rede
    rede, carregador, historico_perda, historico_acuracia = treinar_rede_neural()
    
    # Testar com nova imagem
    testar_com_imagem_nova(rede, carregador)
    
    print("\nğŸ‰ PROGRAMA CONCLUÃDO COM SUCESSO!")
    print("ğŸ’¡ Dica: Experimente ajustar parÃ¢metros para melhorar a performance!")